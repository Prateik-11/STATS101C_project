{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gj2_1GMxAIIE"
   },
   "source": [
    "# Text classification with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3AfrBrP3LYsh",
    "outputId": "147f2240-3b72-4952-a573-76d06488945a"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,embeddings = [],[]\n",
    "\n",
    "with open('/Users/juliabi2020/Desktop/ECE_C147_project/40/model_edited.txt','rt',encoding =\"utf8\", errors = 'ignore') as fi:\n",
    "    full_content = fi.read() # read the file\n",
    "    full_content = full_content.strip() # remove leading and trailing whitespace\n",
    "    full_content = full_content.split('\\n') # split the text into a list of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_1VGpsdNL__r"
   },
   "outputs": [],
   "source": [
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0] # get the word at the start of the line\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:-1]] # get the embedding of the word in an array\n",
    "    # add the word and the embedding to our lists\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[4027162].append(0)\n",
    "embeddings[8054325].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our lists to numpy arrays:\n",
    "import numpy as np\n",
    "vocab_npa = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.377614 -0.283503  0.259995  0.063064  0.363405  0.102221 -0.02912\n",
      " -0.266371 -0.488356 -0.019796  0.484633  0.063539 -0.029836 -0.0357\n",
      " -0.425755  0.328631  0.04247   0.079752  0.033224  0.171046 -0.111937\n",
      "  0.029369  0.318217 -0.364779  0.059668 -0.597157 -0.105175  0.288795\n",
      " -0.108845  0.155113 -0.253356  0.084372 -0.017021  0.056681 -0.49029\n",
      " -0.008505  0.226079 -0.133966  0.088106 -0.087992 -0.161924  0.095436\n",
      " -0.35892   0.080248  0.307295  0.428645  0.443965 -0.142678 -0.279177\n",
      "  0.022955  0.101587 -0.309376 -0.065637  0.264249 -0.204673  0.473244\n",
      "  0.521262  0.065112  0.053894 -0.009417 -0.249188  0.180903 -0.157213\n",
      " -0.017196 -0.465254  0.200362  0.309438  0.012773  0.185225  0.128816\n",
      "  0.134175  0.122978  0.028184 -0.113631 -0.359048 -0.108823 -0.353519\n",
      "  0.189359 -0.333638 -0.120147 -0.276866 -0.042862 -0.144961 -0.142463\n",
      "  0.106302  0.245423  0.287594  0.046989  0.228753  0.20481  -0.021795\n",
      "  0.297307 -0.402078  0.124864  0.132662 -0.218039  0.005076  0.212689\n",
      "  0.258635  0.233413]\n"
     ]
    }
   ],
   "source": [
    "embs_npa = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdbkn8wuRBbR",
    "outputId": "b0f36d72-9d28-477e-85e8-983d5b8be9e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<unk>' 'to' 'a' 'in' '-' ')' '(' ':' 'for']\n",
      "(8054328, 100)\n"
     ]
    }
   ],
   "source": [
    "# insert tokens for padding and unknown words into our vocab\n",
    "vocab_npa = np.insert(vocab_npa, 0, '<pad>')\n",
    "vocab_npa = np.insert(vocab_npa, 1, '<unk>')\n",
    "print(vocab_npa[:10])\n",
    "\n",
    "# make embeddings for these 2:\n",
    "# -> for the '<pad>' token, we set it to all zeros\n",
    "# -> for the '<unk>' token, we set it to the mean of all our other embeddings\n",
    "\n",
    "pad_emb_npa = np.zeros((1, embs_npa.shape[1])) \n",
    "unk_emb_npa = np.mean(embs_npa, axis=0, keepdims=True) \n",
    "\n",
    "#insert embeddings for pad and unk tokens to embs_npa.\n",
    "embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))\n",
    "print(embs_npa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBpGRxt0RGAB",
    "outputId": "6d25ba2a-74b0-40b1-a6b4-ca7649bbfdba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8054328, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "my_embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float())\n",
    "\n",
    "# sanity check\n",
    "assert my_embedding_layer.weight.shape == embs_npa.shape\n",
    "print(my_embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "RrHiL-35UIcZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load train.csv into a pandas dataframe\n",
    "# we can use any other NLP binary classfication problem here as long as\n",
    "# there are only 2 columns: 'review' and 'label'\n",
    "# where review contains text and label contains 0/1\n",
    "\n",
    "df = pd.read_csv(\"/Users/juliabi2020/Desktop/ECE_C147_project/archive/yelp_review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  label\n",
      "0  Super simple place but amazing nonetheless. It...      1\n",
      "1  Small unassuming place that changes their menu...      1\n",
      "2  Lester's is located in a beautiful neighborhoo...      1\n",
      "3  Love coming here. Yes the place always needs t...      1\n",
      "4  Had their chocolate almond croissant and it wa...      1\n"
     ]
    }
   ],
   "source": [
    "df_new = df.drop(columns = ['review_id', 'user_id', 'business_id', 'date', 'useful', 'funny', 'cool'])\n",
    "df_new['label'] = df_new.apply(lambda row: int(row.stars>=4), axis = 1)\n",
    "#1: 4 or 5 stars, 0: <4 stars\n",
    "df_new = df_new.drop(columns = ['stars'])\n",
    "df_new = df_new.rename(columns={\"text\": \"review\"})\n",
    "\n",
    "print(df_new.head())\n",
    "df = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:       3683167 rows\n",
      "Test:        1052334 rows\n",
      "Validation:  526167 rows\n"
     ]
    }
   ],
   "source": [
    "train_prop = 0.7 # 70% for training set\n",
    "test_prop = 0.2 # 20% for test set\n",
    "val_prop = 0.1 # 10% for validation set\n",
    "\n",
    "# split the data into training and validation sets\n",
    "train_val_df, test_df = train_test_split(df, test_size=test_prop, shuffle=True, random_state=11)\n",
    "df, val_df = train_test_split(train_val_df, test_size=val_prop/(train_prop+val_prop), shuffle=True, random_state=11)\n",
    "\n",
    "# print the number of rows in each set\n",
    "print(f\"Train:       {len(df)} rows\")\n",
    "print(f\"Test:        {len(test_df)} rows\")\n",
    "print(f\"Validation:  {len(val_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "zpk-qQSISpU4"
   },
   "outputs": [],
   "source": [
    "class CNNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, vocab, max_seq_length, pad_token, unk_token):\n",
    "        # make a list of our labels\n",
    "        self.labels = df.label.tolist()\n",
    "\n",
    "        # make a dictionary converting each word to its id in the vocab, as well\n",
    "        # as the reverse lookup\n",
    "        self.word2idx = {term:idx for idx,term in enumerate(vocab)}\n",
    "        self.idx2word = {idx:word for word,idx in self.word2idx.items()} \n",
    "        \n",
    "        self.pad_token,self.unk_token = pad_token,unk_token\n",
    "\n",
    "        self.input_ids = [] \n",
    "        self.sequence_lens = [] \n",
    "        self.labels = []\n",
    "\n",
    "        for i in range(df.shape[0]):\n",
    "            # clean up each sentence and turn it into tensor containing the  \n",
    "            # token ids of each word. Also add padding to make them all the \n",
    "            # same length as the longest sequence\n",
    "            input_ids,sequence_len = self.convert_text_to_input_ids(\n",
    "                df.iloc[i].review,\n",
    "                pad_to_len = max_seq_length) \n",
    "            \n",
    "            self.input_ids.append(input_ids.reshape(-1))\n",
    "            self.sequence_lens.append(sequence_len)\n",
    "            self.labels.append(df.iloc[i].label)\n",
    "        \n",
    "        #sanity checks\n",
    "        assert len(self.input_ids) == df.shape[0]\n",
    "        assert len(self.sequence_lens) == df.shape[0]\n",
    "        assert len(self.labels) == df.shape[0]\n",
    "    \n",
    "    def convert_text_to_input_ids(self,text,pad_to_len):\n",
    "        # truncate excess words (beyond the length we should pad to)\n",
    "        words = text.strip().split()[:pad_to_len]\n",
    "\n",
    "        # add padding till we've reached desired length \n",
    "        deficit = pad_to_len - len(words) \n",
    "        words.extend([self.pad_token]*deficit)\n",
    "\n",
    "        # replace words with their id\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in self.word2idx:\n",
    "                # if word is not in vocab, then use <unk> token\n",
    "                words[i] = self.word2idx[self.unk_token] \n",
    "            else:\n",
    "                # else find the id associated with the word \n",
    "                words[i] = self.word2idx[words[i]] \n",
    "        return torch.Tensor(words).long(),pad_to_len - deficit\n",
    "\n",
    "    def __len__(self):\n",
    "        # Make dataset compatible with len() function\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # for the ith indexm return a dictionary containing id, length and label\n",
    "        sample_dict = dict()\n",
    "        sample_dict['input_ids'] = self.input_ids[i].reshape(-1)\n",
    "        sample_dict['sequence_len'] = torch.tensor(self.sequence_lens[i]).long()\n",
    "        sample_dict['labels'] = torch.tensor(self.labels[i]).type(torch.FloatTensor)\n",
    "        return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "DHHgjpseZKXq"
   },
   "outputs": [],
   "source": [
    "class CNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        \n",
    "        # use the pretrained embeddings and check whether or not we should\n",
    "        # freeze embeddings from our config dict\n",
    "        pretrained_embeddings = config['pretrained_embeddings'] if 'pretrained_embeddings' in config else None\n",
    "        freeze_embeddings = config['freeze_embeddings'] if 'freeze_embeddings' in config else False\n",
    "        if pretrained_embeddings is not None:\n",
    "            # use pretrained embeddings\n",
    "            self.vocab_size = pretrained_embeddings.shape[0]\n",
    "            self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(\n",
    "                torch.from_numpy(pretrained_embeddings).float(),\n",
    "                freeze=freeze_embeddings\n",
    "                )\n",
    "        else:\n",
    "            # use randomly initialized embeddings\n",
    "            assert 'vocab' in config and 'embedding_dim' in config\n",
    "            self.vocab_size = config['vocab'].shape[0]\n",
    "            self.embedding_dim = config['embedding_dim']\n",
    "            if freeze_embeddings:\n",
    "                # why would you do this?\n",
    "                print(\n",
    "                    'WARNING:Freezing Randomly Initialized Embeddings!!ðŸ˜­ðŸ˜­ðŸ˜­'\n",
    "                    )\n",
    "            self.embedding = torch.nn.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.embedding_dim,\n",
    "                freeze = freeze_embeddings\n",
    "                )\n",
    "        self.kernel_size = 93\n",
    "        self.out_channels = 32\n",
    "        \n",
    "        # store some values from the config \n",
    "        self.hidden_size = config['hidden_size']\n",
    "\n",
    "        self.cnn = torch.nn.Conv1d(\n",
    "            in_channels = self.embedding_dim,\n",
    "            out_channels = self.out_channels,\n",
    "            kernel_size = self.kernel_size\n",
    "            )\n",
    "        \n",
    "        middle_nodes = int(self.hidden_size / 2)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_features = self.hidden_size, out_features = middle_nodes)\n",
    "        self.fc2 = torch.nn.Linear(in_features = middle_nodes, out_features = 1)\n",
    "        self.relu = torch.nn.functional.relu\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch['input_ids'].to(device) # lookup token ids for our inputs\n",
    "        x_lengths = batch['sequence_len'] # lookup lengths of our inputs\n",
    "        embed_out = self.embedding(x) # get the embeddings of the token ids\n",
    "\n",
    "        embed_out = embed_out.permute(0,2,1)\n",
    "        \n",
    "        cnn_out = self.cnn(embed_out)\n",
    "\n",
    "        m_flatten = nn.Flatten()\n",
    "        fc1_in = m_flatten(cnn_out)\n",
    "        \n",
    "        #in_features= self.hidden_size\n",
    "        fc1_out = self.fc1(fc1_in)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        fc2_out = self.fc2(fc1_out)\n",
    "        final_out = self.sigmoid(fc2_out)\n",
    "        return final_out\n",
    "    \n",
    "    def get_embedding_dims(self):\n",
    "        return self.vocab_size, self.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "xCVdov26beF4"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    #model configurations\n",
    "    'batch_size':32,\n",
    "    'max_seq_length':100,\n",
    "    'lr':1e-4,\n",
    "    'label_count':2,\n",
    "    'dropout_prob':2e-1,\n",
    "    'hidden_size':256,\n",
    "\n",
    "    #embeddings configurations\n",
    "    'pretrained_embeddings':embs_npa,\n",
    "    'freeze_embeddings':True,\n",
    "    'vocab':vocab_npa,\n",
    "    'pad_token':'<pad>',\n",
    "    'unk_token':'<unk>',\n",
    "\n",
    "    #data\n",
    "    'train_df': df, #TODO: set val and test to appropriate\n",
    "    'val_df': val_df,\n",
    "    'test_df': test_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "klDnR0Wxb03n"
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = CNNDataset(\n",
    "    df = config['train_df'], \n",
    "    vocab = config['vocab'],\n",
    "    max_seq_length = config['max_seq_length'],\n",
    "    pad_token = config['pad_token'],\n",
    "    unk_token = config['unk_token']\n",
    ")\n",
    "val_dataset = CNNDataset(\n",
    "    df = config['val_df'], \n",
    "    vocab = config['vocab'],\n",
    "    max_seq_length = config['max_seq_length'],\n",
    "    pad_token = config['pad_token'],\n",
    "    unk_token = config['unk_token']\n",
    ")\n",
    "test_dataset = CNNDataset(\n",
    "    df = config['test_df'], \n",
    "    vocab = config['vocab'],\n",
    "    max_seq_length = config['max_seq_length'],\n",
    "    pad_token = config['pad_token'],\n",
    "    unk_token = config['unk_token']\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = config['batch_size'], shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = config['batch_size'], shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CNNEncoder(config)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "loss_criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "x_bQcvNepZOu"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def validation(model, val_loader):\n",
    "    with torch.no_grad():\n",
    "        all_outputs = []\n",
    "        all_labels = []\n",
    "        for data in val_loader:\n",
    "            labels = data['labels'].to(device)\n",
    "            outputs = model(data)\n",
    "            all_outputs = all_outputs + torch.round(outputs.squeeze()).tolist()\n",
    "            all_labels = all_labels + labels.tolist()\n",
    "        \n",
    "        accuracy = sum([i==j for i, j in zip(all_outputs, all_labels)]) / len(all_labels)\n",
    "        f1 = f1_score(y_pred= all_outputs, y_true=all_labels)\n",
    "\n",
    "        return accuracy, f1\n",
    "\n",
    "def testing(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        all_outputs = []\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        for data in test_loader:\n",
    "            labels = data['labels'].to(device)\n",
    "            outputs = model(data)\n",
    "            all_scores = all_scores + (outputs.squeeze()).tolist()\n",
    "            all_outputs = all_outputs + torch.round(outputs.squeeze()).tolist()\n",
    "            all_labels = all_labels + labels.tolist()\n",
    "        accuracy = sum([i==j for i, j in zip(all_outputs, all_labels)]) / len(all_labels)\n",
    "        f1 = f1_score(y_pred= all_outputs, y_true=all_labels)\n",
    "        roc = roc_auc_score(y_score= all_scores, y_true=all_labels)\n",
    "        cm = confusion_matrix(all_labels, all_outputs)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap = 'magma')\n",
    "        plt.show()\n",
    "        print('Test Statistics:')\n",
    "        print()\n",
    "        print(f\"Accuracy     : {accuracy}\")\n",
    "        print(f\"F1 score     : {f1}\")\n",
    "        print(f\"AUC ROC score: {roc}\")\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "s8VhpNQdoQKV"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45cb47fd87e4ed093c410b1f5ca2956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=115099.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-fc3c0c6da48b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "  progress_bar = tqdm(train_dataloader, leave=False)\n",
    "  losses = []\n",
    "  accuracies = []\n",
    "  total = 0\n",
    "  for data in progress_bar:\n",
    "    target = data['labels'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    \n",
    "    #print(output.shape)\n",
    "    #print(target.shape)\n",
    "    \n",
    "    loss = loss_criterion(output.squeeze(), target)\n",
    "    \n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "    optimizer.step()\n",
    "    accuracy = torch.sum(target == torch.round(output.squeeze())) / target.shape[0]\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    accuracies.append(accuracy.item())\n",
    "    total += 1\n",
    "\n",
    "    progress_bar.set_description(f'Loss: {loss.item():.3f}, Train Accuracy: {accuracy:.3f}')\n",
    "  \n",
    "  val_accuracy, val_f1 = validation(model, val_dataloader)\n",
    "  print(f'Epoch: {epoch}')\n",
    "  print(f'Training   | Loss: {(sum(losses) / total):.4f} | Accuracy: {(sum(accuracies) / total):.2f}% ')\n",
    "  print(f'Validation | F1:   {val_f1:.4f} | Accuracy: {val_accuracy:.2f}% ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX-i1DiOc_SG"
   },
   "source": [
    "Now we will test our model using the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17VkY0uscvnV"
   },
   "outputs": [],
   "source": [
    "testing(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f96c0a065a6f038b4d81e02ec5e62503dd3b71442db074018719e130d8db16de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
